{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/Atharva-Malode/ML-Bootcamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"/content/ML-Bootcamp/Week-1/Day-2/plots.py\" \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "%matplotlib widget\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plots import plot_regression_line, visualize_data, plot_real_cost_function, soup_bowl_3D, soup_bowl_2D, soup_bowl_3D_interactive, visualize_gradient, visualize_gradient_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100)\n",
    "Y = 2 * X + 1 + np.random.randn(100) * 0.1\n",
    "\n",
    "# Set initial parameter values\n",
    "w_initial = 0\n",
    "b_initial = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cost\n",
    "The equation for cost with one variable is:\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
    " \n",
    "where \n",
    "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n",
    "  \n",
    "- $f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.  \n",
    "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   \n",
    "- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples\n",
    "      y (ndarray (m,)): target vlllllllalues\n",
    "      w,b (scalar)    : model parameters\n",
    "\n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    cost_sum = 0\n",
    "    for i in range(None):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum = None + cost\n",
    "    total_cost = (1 / (None)) * cost_sum\n",
    "    return total_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute Gradient\n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{3}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{4}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples\n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters\n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "     \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(None):\n",
    "        f_wb = None\n",
    "        dj_dw_i = (None) * x[i]\n",
    "        dj_db_i = f_wb - y[i]\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += None\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Update the parameters <code>num_iterations</code> times\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w, b. Updates w, b by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "        x (ndarray (m,)): Data, m examples\n",
    "        y (ndarray (m,)): Target values\n",
    "        w_in, b_in (scalar): Initial values of model parameters\n",
    "        alpha (float): Learning rate\n",
    "        num_iters (int): Number of iterations to run gradient descent\n",
    "        cost_function: Function to compute the cost\n",
    "        gradient_function: Function to compute the gradient\n",
    "\n",
    "    Returns:\n",
    "        w (scalar): Updated value of the parameter w after running gradient descent\n",
    "        b (scalar): Updated value of the parameter b after running gradient descent\n",
    "        J_history (list): History of cost values\n",
    "        p_history (list): History of parameters [w, b]\n",
    "    \"\"\"\n",
    "    # An array to store cost J and parameter values at each iteration\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update parameters using gradient descent\n",
    "        b -= None * dj_db\n",
    "        w -= None * dj_dw\n",
    "\n",
    "        #The rest of code is for plotting and displaying the output\n",
    "        # Save cost J and parameter values at each iteration\n",
    "        J_history.append((x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "\n",
    "        # Print cost and parameter values at regular intervals\n",
    "        if i % (num_iters // 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \"\n",
    "                  f\"w: {w.item(): 0.3e}, b: {b.item(): 0.5e}\")\n",
    "\n",
    "    # Plot the cost function\n",
    "    plt.plot(J_history)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Gradient Descent: Cost vs. Iteration')\n",
    "    plt.show()\n",
    "\n",
    "    return w, b, J_history, p_history\n",
    "\n",
    "# Example usage\n",
    "w_final, b_final, J_history, p_history = gradient_descent(X, Y, w_initial, b_initial, 0.01, 1000, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "        x (ndarray (m,))  : Data, m examples\n",
    "        y (ndarray (m,))  : target values\n",
    "        w_in,b_in (scalar): initial values of model parameters\n",
    "        alpha (float):     Learning rate\n",
    "        num_iters (int):   number of iterations to run gradient descent\n",
    "        cost_function:     function to call to produce cost\n",
    "        gradient_function: function to call to produce gradient\n",
    "\n",
    "    Returns:\n",
    "        w (scalar): Updated value of parameter after running gradient descent\n",
    "        b (scalar): Updated value of parameter after running gradient descent\n",
    "        J_history (List): History of cost values\n",
    "        p_history (list): History of parameters [w,b]\n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw.item(): 0.3e}, dj_db: {dj_db.item(): 0.3e}  \",\n",
    "                  f\"w: {w.item(): 0.3e}, b:{b.item(): 0.5e}\")\n",
    "\n",
    "    # Plot 3D graph\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Create a grid of w and b values\n",
    "    w_vals = np.linspace(-1, 3, 100)\n",
    "    b_vals = np.linspace(-1, 3, 100)\n",
    "    W, B = np.meshgrid(w_vals, b_vals)\n",
    "\n",
    "    # Compute the cost for each combination of w and b\n",
    "    cost_vals = np.zeros_like(W)\n",
    "    for i in range(len(w_vals)):\n",
    "        for j in range(len(b_vals)):\n",
    "            cost_vals[i, j] = cost_function(x, y, W[i, j], B[i, j])\n",
    "\n",
    "    # Plot the 3D surface plot of the cost function\n",
    "    ax.plot_surface(W, B, cost_vals, cmap='viridis', alpha=0.8)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('b')\n",
    "    ax.set_zlabel('Cost')\n",
    "    ax.set_title('Gradient Descent')\n",
    "\n",
    "    # Plot the history of parameter updates\n",
    "    w_history, b_history = zip(*p_history)\n",
    "    ax.plot(w_history, b_history, J_history, color='red', marker='o')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "gradient_descent(X, Y, w_initial, b_initial, alpha=0.01, num_iters=100, cost_function=compute_cost, gradient_function=compute_gradient)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
