# -*- coding: utf-8 -*-
"""Multivariate_Linear_Rgression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Atharva-Malode/ML-Bootcamp/blob/master/Week-2/Day-1/Multivariate_Linear_Rgression.ipynb
"""

!pip install ipympl

!git clone "https://github.com/Atharva-Malode/ML-Bootcamp.git"

!cp "/content/ML-Bootcamp/Week-2/Day-1/plots_week2.py" "/content/"

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
from ipywidgets import interact
# %matplotlib widget
from google.colab import output
output.enable_custom_widget_manager()
from mpl_toolkits.mplot3d import Axes3D
import copy
from matplotlib import animation
from plots_week2 import plot_3d_graph, cost_vs_iteration

#DO NOT EDIT THIS CELL
# Set random seed for reproducibility
np.random.seed(42)

# Generate random dataset for each feature
m = 100  # Number of examples
study_hours = np.random.randint(1, 8, size=m)  # Random study hours ranging from 1 to 7
math_score = np.random.randint(40, 100, size=m)  # Random math scores ranging from 40 to 99
science_score = np.random.randint(50, 100, size=m)  # Random science scores ranging from 50 to 99
english_score = np.random.randint(30, 90, size=m)  # Random English scores ranging from 30 to 89
attendance_percentage = np.random.randint(70, 100, size=m)  # Random attendance percentages ranging from 70 to 99

# Generate random target variable (student rank)
y = np.random.randint(1, 6, size=m)  # Random student ranks ranging from 1 to 5

# Provide explanations for each feature
feature_explanations = {
    'study_hours': study_hours,
    'math_score': math_score,
    'science_score': science_score,
    'english_score': english_score,
    'attendance_percentage': attendance_percentage,
    "Student Rank": y,
}
# Create a DataFrame using the dictionary
dataset = pd.DataFrame(feature_explanations)

alpha = 9e-7  # Learning rate
num_iters = 100  # Number of iterations
X = dataset.drop("Student Rank", axis=1)
Y = dataset['Student Rank']
# Initialize initial values for w and b
n_features = X.shape[1]
w_initial = np.zeros((n_features,))
b_initial = 0.0

#Lets Visualize out dataset
dataset #as this is a notebook we do not need to use print function

dataset.describe()

X

Y

#DO NOT EDIT THIS CELL
plot_3d_graph(X.values, Y.values)

((w*x)+b) - y

"""
# Calculating the error
The Error for every example is calculated as:

$$\text{error}^{(i)} = f_w(x^{(i)}) - y^{(i)}$$

where,
* $f_w(x^{(i)}) = w_0 + w_1x_1^{(i)} + w_2x_2^{(i)} + \ldots + w_nx_n^{(i)}$"""

def compute_error(x, y, w, b):
    """
    Computes the error for linear regression
    Args:
      x (ndarray (n,)): Data for a single example with n features
      y (scalar)       : target value
      w (ndarray (n,)) : model parameters
      b (scalar)       : model parameter

    Returns:
      error (scalar): The error between predicted value and target value.
    """
    multiplication = np.dot(x, w)
    add_bias = multiplication + b
    subtract_actual_value = add_bias - y
    error = subtract_actual_value
    return error

"""# Compute cost with multiple variables
The equation for the cost function with multiple variables $J(\mathbf{w},b)$ is:
$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (\hat{y}^{(i)} - y^{(i)})^2 \tag{1}$$
"""

def compute_cost(X, y, w, b):
    """
    Compute the cost function for linear regression.

    Args:
        X (ndarray): Input features (m x n)
        y (ndarray): Target values (m x 1)
        w (ndarray): Model parameters (n x 1)
        b (float): Model bias (scalar)

    Returns:
        cost (float): Computed cost value
    """
    m = len(y)  # Number of examples
    total_cost = 0.0

    for i in range(m):
        # Get the i-th example and target value
        x_i = X[i]
        y_i = y[i]

        ### START CODE HERE##
        # Compute the error for the i-th example
        error = compute_error(x_i, y_i, w, b)              #Hint: use the computer_error function implemented above

        # Compute the squared error
        squared_error = error ** 2

        # Accumulate the squared error to the total cost
        total_cost += squared_error

    # Divide the total cost by 2m to get the average cost
    cost = total_cost / (2 * m)
    ###END CODE HERE###
    return cost

"""# 2 Gradient Descent With Multiple Variables

## 2.2 Calculating Gradient

$$
\begin{align}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (\text{error}^{(i)})x_{j}^{(i)} \tag{3}  \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (\text{error}^{(i)}) \tag{4}
\end{align}
$$
* m is the number of training examples in the data set


*  $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value
"""

def compute_gradient(X, y, w, b):
    """
    Computes the gradient for linear regression
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters
      b (scalar)       : model parameter

    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.
    """
    m, n = X.shape
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):
        error = compute_error(X[i], y[i], w, b)          #Hint: use the computer_error function implemented above

        for j in range(n):
            dj_dw[j] += error * X[i, j]
        dj_db += error

    dj_dw /= m
    dj_db /= m

    return dj_db, dj_dw

"""## 2.3 Gradient descent for multiple variables:

$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline\;
& w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}  \; & \text{for j = 0..n-1}\newline
&b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}$$

where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously
"""

def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):
    """
    Performs batch gradient descent to learn w and b. Updates w and b by taking
    num_iters gradient steps with learning rate alpha

    Args:
      X (ndarray (m,n))   : Data, m examples with n features
      y (ndarray (m,))    : target values
      w_in (ndarray (n,)) : initial model parameters
      b_in (scalar)       : initial model parameter
      cost_function       : function to compute cost
      gradient_function   : function to compute the gradient
      alpha (float)       : Learning rate
      num_iters (int)     : number of iterations to run gradient descent

    Returns:
      w (ndarray (n,)) : Updated values of parameters
      b (scalar)       : Updated value of parameter
      """

    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in)  #avoid modifying global w within function
    b = b_in

    for i in range(num_iters):
        ### START CODE HERE ###
        # Calculate the gradient and update the parameters
        dj_db,dj_dw = gradient_function(X, y, w, b)   #Hint: Use the gradient_function passed as paramater

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw               ##None
        b = b - alpha * dj_db               ##None
        ### END CODE HERE ###


        # Save cost J at each iteration
        if i<100:      # prevent resource exhaustion
            J_history.append( cost_function(X, y, w, b))

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   ")

    return w, b, J_history #return final w,b and J history for graphing

# Perform gradient descent
w_final, b_final, J_history = gradient_descent(X.values, Y.values, w_initial, b_initial, compute_cost, compute_gradient, alpha, num_iters)

cost_vs_iteration(J_history, num_iters)

